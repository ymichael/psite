webpackJsonp([65429831292191],{359:function(n,e){n.exports={data:{markdownRemark:{html:'<p>A couple of days ago, I had an assignment for an <a href="//www.comp.nus.edu.sg/~kanmy/courses/3245_2014/index.html">Information Retrieval\nclass</a>\nthat basically involved:</p>\n<ol>\n<li>Indexing a large corpus <em>(Reuters from <code class="language-text">nltk</code>)</em></li>\n<li>Searching it using boolean queries <em>(eg. <code class="language-text">bill AND gates</code>)</em></li>\n</ol>\n<p>For the second part of the of the assignment, performance was pretty important.\nSince we would want to return results for the user\'s queries quickly.</p>\n<p><strong>This blogpost is basically about how I used python\'s cProfile to\nidentify and fix bottlenecks/slow parts in my code.</strong> Most of these bottlenecks\nwould have been hard to identify without the profiler.</p>\n<!--more-->\n<p>During my internship at Quora, one of the things I worked on was POST speed\nimprovements for core actions across the product. It was my first brush with\nspeed work and the main lesson I took away was the importance of\nmeasuring and profiling before attempting to optimise.</p>\n<blockquote>\n<p>"Bottlenecks occur ins surprising places, so don\'t try to second guess and put in a speed hack until you have proven that\'s where the bottleneck is." - Rob Pike</p>\n</blockquote>\n<h2>Measure</h2>\n<p>After getting a working submission out. I proceeded to measure/benchmark my code. Since\nthis program was meant to run as a script from the command line, I used the\nsimple <code class="language-text">time</code> command-line tool to roughly benchmark how fast my code was.</p>\n<div class="gatsby-highlight">\n      <pre class="language-bash"><code class="language-bash">$ <span class="token function">time</span> python search.py\n<span class="token comment"># real    0m1.521s</span>\n<span class="token comment"># user    0m1.250s</span>\n<span class="token comment"># sys     0m0.142s</span></code></pre>\n      </div>\n<h2>Profile</h2>\n<p>Once I was happy with the measurement, I proceeded to profile my code using <code class="language-text">cProfile</code>.</p>\n<div class="gatsby-highlight">\n      <pre class="language-bash"><code class="language-bash">$ python -m cProfile -o profile_output search.py</code></pre>\n      </div>\n<p>The <code class="language-text">-o</code> flag basically specifies an output filename for cProfile to write its\noutput to. (Without which, it\'ll simply spit out the stats to the stdout, which\nis undesirable since we\'d be unable to sort/drill down into specific functions.)</p>\n<h2>Making sense of the cProfile output</h2>\n<p>The cProfile output is a basically binary file that contains the following\ninformation:</p>\n<p><em>(For each function called in the python program)</em></p>\n<ul>\n<li>How long each call took (percall, inclusive and exclusive)</li>\n<li>How many times it was called (ncalls)</li>\n<li>How long it took (cumtime: includes the times of other functions it calls)</li>\n<li>How long it actually took (tottime: excludes the times of other functions)</li>\n<li>What functions it called (callers)</li>\n<li>What functions called it (callees)</li>\n</ul>\n<p><strong>If you didn\'t specify the output, you\'ll basically only get a dump of the\ninformation without the caller/callees part. (Which is quite helpful in making\nsense of everything). You\'d also lose the ability to dynamically re-sort the\ninformation based on your preferred metric (unless you re-profile it with a <code class="language-text">-s</code> flag (I think)).</strong></p>\n<h3>Reading the cProfile binary output file</h3>\n<p>In order to read this binary file, python provides a pstats.Stats class that\nhappily spits out the various infomation for you in a python shell (when called\nwith the relevant methods).</p>\n<p>I found this rather tedious and <a href="http://blog.ludovf.net/python-profiling-cprofile/">googling</a> <a href="http://pymotw.com/2/profile/">around</a> for an easier way to read this\nbinary file yield nothing. I just wanted a simple way to:</p>\n<ol>\n<li>See all the information</li>\n<li>Sort them with a single click</li>\n<li>Drill down to functions (and their callers and callees) with a single click</li>\n</ol>\n<p><em>(These as oppose to manually calling methods on the Stats object each time.)</em></p>\n<p>So I wrote this: <a href="https://github.com/ymichael/cprofilev">CProfileV</a>. Which is\nbacially a thin wrapper for viewing python cProfile output in your browser. Yay!</p>\n<p><strong>[UPDATE]: cProfileV has been updated. See <a href="https://github.com/ymichael/cprofilev">https://github.com/ymichael/cprofilev</a> for the latest usage instructions.</strong></p>\n<div class="gatsby-highlight">\n      <pre class="language-bash"><code class="language-bash"><span class="token comment"># Install cprofilev</span>\n$ <span class="token function">sudo</span> pip <span class="token function">install</span> cprofilev\n\n<span class="token comment"># Call it with your cprofile output</span>\n$ cprofilev -f /path/to/cprofile/output\n\n<span class="token comment"># Navigate to http://localhost:4000</span></code></pre>\n      </div>\n<p><img src="/static/img/blog/cprofilev1.png" alt="cprofilev1"></p>\n<h3>Finding the bottlenecks</h3>\n<p>The pstats.Stats object allows you to sort by the various keys and their\ncombinations. Most often, I find that the most useful sort keys are:</p>\n<ul>\n<li>cumulative time</li>\n<li>total time</li>\n<li>calls <em>(to find unneccessary function calls (objects being created in a loop for instance))</em></li>\n</ul>\n<p>Sorting by total time for me yielded this:</p>\n<p><img src="/static/img/blog/cprofilev2-tottime.png" alt="cprofilev2-tottime"></p>\n<p>The top line stood out to me. The function <code class="language-text">not_operation</code> was taking a\nsuspiciously long time.</p>\n<p>Clicking into the <code class="language-text">not_operation</code> showed that the functions it was calling were\nnot taking a lot of time. <strong>(Implying that the slowness was due to some code within the function itself.)</strong></p>\n<p><img src="/static/img/blog/cprofilev3-not-operation.png" alt="cprofilev3-not-operation"></p>\n<div class="gatsby-highlight">\n      <pre class="language-py"><code class="language-py"># line 76 of search_index.py\ndef not_operation(operand, dictionary, pfile):\n    &quot;&quot;&quot;Performs the operation `NOT operand`.&quot;&quot;&quot;\n\n    # A list of all the documents (sorted)\n    all_docs = dictionary.all_docs()\n\n    # A list of the documents matching `operand` (sorted)\n    results = get_results(operand, dictionary, pfile, force_list=True)\n\n    return [doc for doc in all_docs if doc not in results]</code></pre>\n      </div>\n<p>So it turns out that the list comprehension in the function was basically really\n<strong>inefficient</strong>. It became super obvious once I narrowed down that <code class="language-text">not_operation</code>\nwas slow.</p>\n<h3>Optimise/Fix ineffient code</h3>\n<p>Excited to have found a possible bottleneck, I quickly implemented a fix.</p>\n<div class="gatsby-highlight">\n      <pre class="language-py"><code class="language-py"># the fix.\ndef not_operation(operand, dictionary, pfile):\n    &quot;&quot;&quot;Performs the operation `NOT operand`.&quot;&quot;&quot;\n\n    # A list of all the documents (sorted)\n    all_docs = dictionary.all_docs()\n\n    # A list of the documents matching `operand` (sorted)\n    results = get_results(operand, dictionary, pfile, force_list=True)\n\n    return list_a_and_not_list_b(all_docs, results)\n\n\ndef list_a_and_not_list_b(a, b):\n    &quot;&quot;&quot;Returns `a AND NOT b`.\n\n    Both a and b are expected to be sorted lists.\n\n    &quot;&quot;&quot;\n    results = []\n    idx_a = 0\n    idx_b = 0\n    while idx_a &lt; len(a) and idx_b &lt; len(b):\n        if a[idx_a] &lt; b[idx_b]:\n            results.append(a[idx_a])\n            idx_a += 1\n        elif b[idx_b] &lt; a[idx_a]:\n            idx_b += 1\n        else:\n            idx_a += 1\n            idx_b += 1\n\n    while idx_a &lt; len(a):\n        results.append(a[idx_a])\n        idx_a += 1\n\n    return results</code></pre>\n      </div>\n<h2>Measure again</h2>\n<p>Measuring the time taken showed promising results. With this fix, the time taken\ndropped from roughly 1.5s to 1.15s.</p>\n<div class="gatsby-highlight">\n      <pre class="language-bash"><code class="language-bash"><span class="token function">time</span> python search.py\n<span class="token comment"># real    0m1.160s</span>\n<span class="token comment"># user    0m1.018s</span>\n<span class="token comment"># sys     0m0.133s</span></code></pre>\n      </div>\n<p>Profiling the code showed that the slowness was no longer coming from\n<code class="language-text">not_operation</code>.</p>\n<p><img src="/static/img/blog/cprofilev4-not-operation.png" alt="cprofilev4-not-operation"></p>\n<h2>Rinse and repeat</h2>\n<p>This time though, the most of the time seemed to be spent in the\n<code class="language-text">list_a_and_not_list_b</code> operation.</p>\n<p><img src="/static/img/blog/cprofilev5-bool-operation.png" alt="cprofilev5-bool-operation"></p>\n<p>In particular, I seemed to be doing ~200k <code class="language-text">len</code> operations and ~115 <code class="language-text">append</code>\noperations on some list objects.</p>\n<p>This seemed like a red-flag so I took a closer look at the\n<code class="language-text">list_a_and_not_list_b</code> function.</p>\n<p>It turns out that I really didn\'t need the while loop at the end of the function.</p>\n<div class="gatsby-highlight">\n      <pre class="language-python"><code class="language-python"><span class="token comment"># From this.</span>\n<span class="token keyword">while</span> idx_a <span class="token operator">&lt;</span> <span class="token builtin">len</span><span class="token punctuation">(</span>a<span class="token punctuation">)</span><span class="token punctuation">:</span>\n    results<span class="token punctuation">.</span>append<span class="token punctuation">(</span>a<span class="token punctuation">[</span>idx_a<span class="token punctuation">]</span><span class="token punctuation">)</span>\n    idx_a <span class="token operator">+=</span> <span class="token number">1</span>\n\n<span class="token comment"># To this.</span>\nresults<span class="token punctuation">.</span>extend<span class="token punctuation">(</span>a<span class="token punctuation">[</span>idx_a<span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token punctuation">)</span></code></pre>\n      </div>\n<p>Measuring with this new change:</p>\n<div class="gatsby-highlight">\n      <pre class="language-bash"><code class="language-bash"><span class="token function">time</span> python search.py\n<span class="token comment"># real    0m0.895s</span>\n<span class="token comment"># user    0m0.771s</span>\n<span class="token comment"># sys     0m0.122s</span></code></pre>\n      </div>\n<p><em>Woot! Got it to run under a second!</em></p>\n<h2>Summary</h2>\n<p>Overall, I\'m pretty happy with the performance gain I got. I did a couple more that I didn\'t cover here (some other boolean operations were performing poorly under other testcases). Its a pretty cool feeling to methodically find bottlenecks and fix them.</p>',frontmatter:{title:"Profiling python with cProfile",date:"08 March, 2014"},fields:{slug:"/2014/03/08/profiling-python-with-cprofile.html"}}},pathContext:{slug:"/2014/03/08/profiling-python-with-cprofile.html"}}}});
//# sourceMappingURL=path---2014-03-08-profiling-python-with-cprofile-html-5957a4582fdd8366a282.js.map